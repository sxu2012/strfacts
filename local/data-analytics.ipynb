{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Jupyter Notebook with Python 3 on Windows 11\n",
    "## Extract the top most frequent used words in reviews for each property listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"analytics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema for reviews.csv.gz\n",
    "reviews_schema = StructType([\n",
    "    StructField('location', StringType(), True),\n",
    "    StructField('listing_id', StringType(), True),\n",
    "    #StructField('id',         IntegerType(),True),\n",
    "    StructField('date',       StringType(),True),\n",
    "    #StructField('reviewer_id', IntegerType(),True),\n",
    "    StructField('reviewer_name', StringType(),True),\n",
    "    StructField('comments', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read reviews parquet file into data frame reviews_df\n",
    "cleandata = 'c:/sb/strfacts/cleandata'\n",
    "reviews_df = spark.read.format(\"parquet\").schema(reviews_schema).load(cleandata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+--------------------+--------+\n",
      "|listing_id|      date|reviewer_name|            comments|location|\n",
      "+----------+----------+-------------+--------------------+--------+\n",
      "|     13528|2010-01-27|         Mike| Very enjoyable e...|  hawaii|\n",
      "|    729224|2013-10-22|        Stacy| \"We spend an ama...|  hawaii|\n",
      "|    729224|2013-12-01|      Shannon| \"My husband  dau...|  hawaii|\n",
      "|    729224|2013-12-28|       Nicole| \"Very well maint...|  hawaii|\n",
      "|    729224|2014-06-10|       Ramona| \"My family spent...|  hawaii|\n",
      "+----------+----------+-------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+\n",
      "|listing_id|        all_comments|  area|\n",
      "+----------+--------------------+------+\n",
      "|  10013402| Thank you Jack a...|hawaii|\n",
      "|  10089978| \"We had a wonder...|hawaii|\n",
      "+----------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by list_id, concat all comments\n",
    "reviews_df.createOrReplaceTempView(\"reviews\")\n",
    "query = \"\"\"\n",
    "    SELECT listing_id, CONCAT_WS(' ', collect_list(comments)) as all_comments, first(location) as area\n",
    "    FROM reviews\n",
    "    GROUP BY listing_id\n",
    "    \"\"\"\n",
    "tmpdf = spark.sql(query)\n",
    "tmpdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex remove all nonalphanumeric characters, split by \" \" and explode words\n",
    "df1 = tmpdf.select(tmpdf.listing_id,tmpdf.area, F.explode(F.split(F.regexp_replace(tmpdf.all_comments,\"[^a-zA-Z0-9 -]\",\"\"),\" \")).alias(\"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------+\n",
      "|listing_id|  area|        words|\n",
      "+----------+------+-------------+\n",
      "|  10013402|hawaii|             |\n",
      "|  10013402|hawaii|        Thank|\n",
      "|  10013402|hawaii|          you|\n",
      "|  10013402|hawaii|         Jack|\n",
      "|  10013402|hawaii|          and|\n",
      "|  10013402|hawaii|         Nida|\n",
      "|  10013402|hawaii|          for|\n",
      "|  10013402|hawaii|        being|\n",
      "|  10013402|hawaii|         very|\n",
      "|  10013402|hawaii|accommodating|\n",
      "+----------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 5 most frequent used words in the comments\n",
    "df2 = df1.groupBy(df1.listing_id, df1.area, df1.words).agg(F.count(df1.words).alias(\"cnt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+---+\n",
      "|listing_id|  area|words|cnt|\n",
      "+----------+------+-----+---+\n",
      "|  10013402|hawaii|     | 37|\n",
      "|  10013402|hawaii|Thank|  1|\n",
      "|  10013402|hawaii|  you|  1|\n",
      "|  10013402|hawaii| Jack| 10|\n",
      "|  10013402|hawaii|  and| 43|\n",
      "+----------+------+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+---+----------+\n",
      "|listing_id|  area|words|cnt|row_number|\n",
      "+----------+------+-----+---+----------+\n",
      "|  10013402|hawaii|Thank|  1|         1|\n",
      "|  10013402|hawaii|  you|  1|         2|\n",
      "|  10013402|hawaii|being|  1|         3|\n",
      "+----------+------+-----+---+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(df2.listing_id).orderBy(df2.cnt)\n",
    "df3 = df2.withColumn(\"row_number\",F.row_number().over(windowSpec))\n",
    "df3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------+---+----------+\n",
      "|listing_id|  area|        words|cnt|row_number|\n",
      "+----------+------+-------------+---+----------+\n",
      "|  10013402|hawaii|        Thank|  1|         1|\n",
      "|  10013402|hawaii|          you|  1|         2|\n",
      "|  10013402|hawaii|        being|  1|         3|\n",
      "|  10013402|hawaii|accommodating|  1|         4|\n",
      "|  10013402|hawaii|        funny|  1|         5|\n",
      "+----------+------+-------------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.select(\"*\").where(df3.row_number<=5)\n",
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.select(df4.listing_id, df4.area, df4.words).groupBy(df4.listing_id, df4.area).agg(F.collect_list(df4.words).alias(\"frequentw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+\n",
      "|listing_id|   area|           frequentw|\n",
      "+----------+-------+--------------------+\n",
      "|  10013402| hawaii|[Thank, you, bein...|\n",
      "|  10089978| hawaii|[showerfelt, perf...|\n",
      "|   1012360| hawaii|[week, breathtaki...|\n",
      "|  10123910|seattle|[Easily, accommod...|\n",
      "|   1014157| hawaii|[updates, giant, ...|\n",
      "+----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.select(df5.listing_id, df5.area, F.array_join(df5.frequentw, \",\").alias(\"most_frequent_used_words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------------------+\n",
      "|listing_id|   area|most_frequent_used_words|\n",
      "+----------+-------+------------------------+\n",
      "|  10013402| hawaii|    Thank,you,being,a...|\n",
      "|  10089978| hawaii|    showerfelt,perfec...|\n",
      "|   1012360| hawaii|    week,breathtaking...|\n",
      "|  10123910|seattle|    Easily,accommodat...|\n",
      "|   1014157| hawaii|    updates,giant,tv,...|\n",
      "|  10158447| hawaii|    ideally,got,under...|\n",
      "|  10210560| hawaii|    Many,thanks,swimm...|\n",
      "|  10214497| hawaii|    Aloha,LOVED,Being...|\n",
      "|  10217927| hawaii|    AMAZING,sensitive...|\n",
      "|   1022135|seattle|    just,superb,beaut...|\n",
      "+----------+-------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyticsdata = 'c:/sb/strfacts/analyticsdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.write.partitionBy(\"area\").mode(\"append\").parquet(analyticsdata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
